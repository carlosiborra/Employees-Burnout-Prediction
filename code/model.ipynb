{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1. Aprendizaje Automático\n",
    "\n",
    "Authors: Carlos Iborra Llopis (100451170), Alejandra Galán Arrospide (100451273) <br>\n",
    "For additional notes and requirements: https://github.com/carlosiborra/Grupo02-Practica2-AprendizajeAutomatico\n",
    "\n",
    "❗If you are willing to run the code yourself, please clone the full GitHub repository, as it contains the necessary folder structures to export images and results❗"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Table of contents\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing necessary libraries \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from matplotlib.cbook import boxplot_stats as bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Cleaning ../data/img/ folder\n",
    "This way we avoid creating multiple images and sending the old ones to the trash.<br>\n",
    "Also using this to upload cleaner commits to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cleaning the ../data/img/ folder \"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"../data/img/*\")\n",
    "for f in files:\n",
    "    if os.path.isfile(f) and f.endswith(\".png\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Reading the datasets\n",
    "Reading the datasets from the bz2 files, group 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/attrition_available_2.pkl', 'rb') as pkl_file:\n",
    "    datos = pickle.load(pkl_file)\n",
    "\n",
    "datos # TODO: FIX THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to a csv file\n",
    "datos.to_csv('../data/attrition_available_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0,15 puntos) Hacer un EDA muy simplificado: cuántas instancias / cuantos \n",
    "atributos y de qué tipo (numéricos, ordinales, categóricos); columnas constantes o \n",
    "innecesarias; que proporción de missing values por atributo; tipo de problema: \n",
    "(clasificación o regresión); ¿es desbalanceado?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts of Exploratory Data Analysis**\n",
    "\n",
    "- **2 types of Data Analysis**\n",
    "  - Confirmatory Data Analysis\n",
    "  - Exploratory Data Analysis\n",
    "- **4 Objectives of EDA**\n",
    "  - Discover Patterns\n",
    "  - Spot Anomalies\n",
    "  - Frame Hypothesis\n",
    "  - Check Assumptions\n",
    "- **2 methods for exploration**\n",
    "  - Univariate Analysis\n",
    "  - Bivariate Analysis\n",
    "- **Stuff done during EDA**\n",
    "  - Trends\n",
    "  - Distribution\n",
    "  - Mean\n",
    "  - Median\n",
    "  - Outlier\n",
    "  - Spread measurement (SD)\n",
    "  - Correlations\n",
    "  - Hypothesis testing\n",
    "  - Visual Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Dataset preparation\n",
    "\n",
    "To conduct exploratory data analysis (EDA) on our real data, we need to prepare the data first. Therefore, we have decided to separate the data into training and test sets at an early stage to avoid data leakage, which could result in an overly optimistic evaluation of the model, among other consequences. This separation will be done by dividing the data prematurely into training and test sets since potential data leakage can occur from the usage of the test partition, especially when including the result variable.\n",
    "\n",
    "It is important to note that this step is necessary because all the information obtained in this section will be used to make decisions such as dimensionality reduction. Furthermore, this approach makes the evaluation more realistic and rigorous since the test set is not used until the end of the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.1. Stratified K-Fold Cross-Validation\n",
    "To ensure a fair and unbiased evaluation of our model's performance, we will be using stratified k-fold for dividing our data into training and test sets. Stratified k-fold is a commonly used technique in machine learning that ensures that the distribution of classes in the training and test sets is similar, thus reducing the risk of introducing bias into our model's performance evaluation.\n",
    "\n",
    "By using stratified k-fold, we can ensure that each fold of the data used for training and testing our model contains a representative sample of all the classes in the dataset. This helps to account for any potential class imbalance in the data, ensuring that our model is trained and tested on a diverse set of data, leading to a more reliable evaluation of its performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as we are second group, we will be using 2 as our random state seed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Simplemente dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train Test Split using Stratified K-Fold \"\"\"\n",
    "\n",
    "# Make a copy of the data (we will re-split the data later to ensure that the data is not contaminated)\n",
    "datos_copy = datos.copy()\n",
    "\n",
    "# Define the number of folds for stratified k-fold cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize the StratifiedKFold object\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over the stratified k-fold splits\n",
    "for train_index, test_index in stratified_kfold.split(datos_copy, datos_copy['Attrition']):\n",
    "    # Split the data into training and test sets using the current split indices\n",
    "    train_set = datos_copy.iloc[train_index]\n",
    "    test_set = datos_copy.iloc[test_index]\n",
    "    \n",
    "    # Extract the features (X) and target (y) from the training and test sets\n",
    "    X_train = train_set.drop('Attrition', axis=1)  # Drop the 'Attrition' column to get the features\n",
    "    y_train = train_set['Attrition']  # Extract the 'Attrition' column as the target\n",
    "    \n",
    "    X_test = test_set.drop('Attrition', axis=1)  # Drop the 'Attrition' column to get the features\n",
    "    y_test = test_set['Attrition']  # Extract the 'Attrition' column as the target\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the train-test division correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "\n",
      "          hrs  absences  JobInvolvement  PerformanceRating  \\\n",
      "1    7.315971      13.0             2.0                4.0   \n",
      "6    6.450877      17.0             NaN                4.0   \n",
      "13   8.871421      14.0             2.0                3.0   \n",
      "28  10.713066       6.0             2.0                3.0   \n",
      "30   9.662808      11.0             2.0                3.0   \n",
      "\n",
      "    EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance   Age  \\\n",
      "1                       3.0              NaN              4.0  31.0   \n",
      "6                       1.0              3.0              1.0  28.0   \n",
      "13                      1.0              2.0              2.0   NaN   \n",
      "28                      4.0              3.0              1.0  44.0   \n",
      "30                      1.0              2.0              3.0  26.0   \n",
      "\n",
      "       BusinessTravel              Department  ...  NumCompaniesWorked  \\\n",
      "1   Travel_Frequently  Research & Development  ...                 0.0   \n",
      "6       Travel_Rarely  Research & Development  ...                 2.0   \n",
      "13         Non-Travel  Research & Development  ...                 NaN   \n",
      "28  Travel_Frequently  Research & Development  ...                 3.0   \n",
      "30      Travel_Rarely  Research & Development  ...                 NaN   \n",
      "\n",
      "    Over18 PercentSalaryHike  StandardHours  StockOptionLevel  \\\n",
      "1        Y              23.0            NaN               1.0   \n",
      "6        Y              20.0            8.0               1.0   \n",
      "13       Y               NaN            8.0               2.0   \n",
      "28       Y               NaN            NaN               NaN   \n",
      "30       Y              11.0            8.0               0.0   \n",
      "\n",
      "   TotalWorkingYears  TrainingTimesLastYear YearsAtCompany  \\\n",
      "1                6.0                    3.0            5.0   \n",
      "6                5.0                    2.0            0.0   \n",
      "13              10.0                    NaN           10.0   \n",
      "28              19.0                    2.0            1.0   \n",
      "30               5.0                    5.0            NaN   \n",
      "\n",
      "   YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "1                      1.0                     4  \n",
      "6                      0.0                     0  \n",
      "13                     9.0                     9  \n",
      "28                     0.0                     0  \n",
      "30                     0.0                     2  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "--------------------\n",
      "1     Yes\n",
      "6     Yes\n",
      "13    Yes\n",
      "28    Yes\n",
      "30    Yes\n",
      "Name: Attrition, dtype: object\n",
      "--------------------\n",
      "           hrs  absences  JobInvolvement  PerformanceRating  \\\n",
      "108   7.100332       NaN             4.0                3.0   \n",
      "112   5.537753      19.0             NaN                3.0   \n",
      "178   6.825260       NaN             NaN                4.0   \n",
      "182  10.465666      12.0             4.0                NaN   \n",
      "221        NaN       3.0             3.0                NaN   \n",
      "\n",
      "     EnvironmentSatisfaction  JobSatisfaction  WorkLifeBalance   Age  \\\n",
      "108                      1.0              2.0              3.0  41.0   \n",
      "112                      3.0              2.0              3.0  21.0   \n",
      "178                      1.0              4.0              3.0  33.0   \n",
      "182                      2.0              1.0              3.0  34.0   \n",
      "221                      1.0              3.0              3.0  24.0   \n",
      "\n",
      "        BusinessTravel              Department  ...  NumCompaniesWorked  \\\n",
      "108  Travel_Frequently  Research & Development  ...                 1.0   \n",
      "112      Travel_Rarely         Human Resources  ...                 1.0   \n",
      "178      Travel_Rarely                   Sales  ...                 7.0   \n",
      "182  Travel_Frequently                   Sales  ...                 NaN   \n",
      "221  Travel_Frequently  Research & Development  ...                 1.0   \n",
      "\n",
      "     Over18 PercentSalaryHike  StandardHours  StockOptionLevel  \\\n",
      "108       Y               NaN            8.0               0.0   \n",
      "112       Y              12.0            8.0               1.0   \n",
      "178     NaN              25.0            8.0               NaN   \n",
      "182       Y              22.0            8.0               1.0   \n",
      "221       Y              21.0            8.0               0.0   \n",
      "\n",
      "    TotalWorkingYears  TrainingTimesLastYear YearsAtCompany  \\\n",
      "108               8.0                    3.0            8.0   \n",
      "112               1.0                    2.0            1.0   \n",
      "178               8.0                    NaN            4.0   \n",
      "182              11.0                    5.0            NaN   \n",
      "221               6.0                    3.0            NaN   \n",
      "\n",
      "    YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "108                     7.0                     7  \n",
      "112                     0.0                     0  \n",
      "178                     1.0                     3  \n",
      "182                     0.0                     2  \n",
      "221                     1.0                     2  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "--------------------\n",
      "108    Yes\n",
      "112    Yes\n",
      "178    Yes\n",
      "182    Yes\n",
      "221    Yes\n",
      "Name: Attrition, dtype: object\n",
      "--------------------\n",
      "\n",
      "(3528, 30), (3528,), (882, 30), (882,)\n",
      "\n",
      "The train test division is correct: 3528 + 882 = 4410\n",
      "\n",
      "--------------------\n",
      "\n",
      "Original dataset class distribution:\n",
      " No     0.838776\n",
      "Yes    0.161224\n",
      "Name: Attrition, dtype: float64\n",
      "\n",
      "--------------------\n",
      "\n",
      "Train set class distribution:\n",
      " No     0.838719\n",
      "Yes    0.161281\n",
      "Name: Attrition, dtype: float64\n",
      "\n",
      "--------------------\n",
      "\n",
      "Test set class distribution:\n",
      " No     0.839002\n",
      "Yes    0.160998\n",
      "Name: Attrition, dtype: float64\n",
      "\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the head of X_train, y_train, X_test, y_test\n",
    "print(\"--------------------\\n\")\n",
    "print(f\"{X_train.head()}\", f\"{y_train.head()}\", f\"{X_test.head()}\", f\"{y_test.head()}\", sep=\"\\n--------------------\\n\")\n",
    "print(\"--------------------\\n\")\n",
    "\n",
    "print(f\"{X_train.shape}, {y_train.shape}, {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Check is division summatory is correct\n",
    "if X_train.shape[0] + X_test.shape[0] == datos_copy.shape[0]:\n",
    "    print(\n",
    "        f\"\\nThe train test division is correct: {X_train.shape[0]} + {X_test.shape[0]} = {datos_copy.shape[0]}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nERROR: The train test division is incorrect\")\n",
    "    \n",
    "    \n",
    "# Check the train-test division correctness by comparing the class distribution in the original dataset and the train and test sets.\n",
    "print(\"\\n--------------------\\n\")\n",
    "print(\"Original dataset class distribution:\\n\", datos_copy['Attrition'].value_counts(normalize=True))\n",
    "print(\"\\n--------------------\\n\")\n",
    "print(\"Train set class distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\n--------------------\\n\")\n",
    "print(\"Test set class distribution:\\n\", y_test.value_counts(normalize=True))\n",
    "print(\"\\n--------------------\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the class distribution is almost perfectly preserved in both the training and test sets and that the sum of the number of rows in train and test give us the total amount of rows in the raw dataset. This is due to the fact that the stratified k-fold algorithm does not guarantee that the class distribution is exactly the same in each fold, but rather that it is similar.\n",
    "\n",
    "Also, we can notice how the distribution of the classes is very similar in both the training and test sets, which is a good sign."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset and problem description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are given contains the following atributes information:\n",
    "\n",
    "0. **hrs**: The number of hours worked by the employee (float64)\n",
    "1. **absences**: The number of absences taken by the employee (float64)\n",
    "2. **JobInvolvement**: The level of involvement the employee has in their job (float64)\n",
    "3. **PerformanceRating**: The employee's performance rating (float64)\n",
    "4. **EnvironmentSatisfaction**: The level of satisfaction the employee has with their work environment (float64)\n",
    "5. **JobSatisfaction**: The level of satisfaction the employee has with their job (float64)\n",
    "6. **WorkLifeBalance**: The balance between work and personal life for the employee (float64)\n",
    "7. **Age**: The age of the employee (float64)\n",
    "8. **Attrition**: Whether the employee has left the company or not (object) -> **Target variable**\n",
    "9. **BusinessTravel**: The frequency of the employee's business travel (object)\n",
    "10. **Department**: The department the employee works in (object)\n",
    "11. **DistanceFromHome**: The distance from the employee's home to their workplace (float64)\n",
    "12. **Education**: The highest level of education attained by the employee (int64)\n",
    "13. **EducationField**: The field of study the employee specialized in (object)\n",
    "14. **EmployeeCount**: The number of employees in the company (float64)\n",
    "15. **EmployeeID**: A unique identifier for each employee (int64)\n",
    "16. **Gender**: The gender of the employee (object)\n",
    "17. **JobLevel**: The employee's job level in the company hierarchy (float64)\n",
    "18. **JobRole**: The specific role the employee has in their department (object)\n",
    "19. **MaritalStatus**: The employee's marital status (object)\n",
    "20. **MonthlyIncome**: The employee's monthly income (float64)\n",
    "21. **NumCompaniesWorked**: The number of companies the employee has worked for before joining the current company (float64)\n",
    "22. **Over18**: Whether the employee is over 18 years old (presumably all employees are) (object)\n",
    "23. **PercentSalaryHike**: The percentage of salary increase the employee received in their last salary hike (float64)\n",
    "24. **StandardHours**: The standard number of working hours in the company (float64)\n",
    "25. **StockOptionLevel**: The level of stock option the employee has (float64)\n",
    "26. **TotalWorkingYears**: The total number of years the employee has worked (float64)\n",
    "28. **TrainingTimesLastYear**: The number of times the employee received training in the last year (float64)\n",
    "29. **YearsAtCompany**: The number of years the employee has been with the company (float64)\n",
    "30. **YearsSinceLastPromotion**: The number of years since the employee's last promotion (float64)\n",
    "31. **YearsWithCurrManager**: The number of years the employee has been with their current manager (float64)\n",
    "\n",
    "**Note**: the values in between parenthesis correspond to the value type of each column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se poseen 31 atributos.\n",
      "Se poseen 4410 instancias.\n"
     ]
    }
   ],
   "source": [
    "num_atributos  = len(datos.keys())\n",
    "print(\"Se poseen\", num_atributos, \"atributos.\")\n",
    "num_instances = len(datos)\n",
    "print(\"Se poseen\", num_instances, \"instancias.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3528 entries, 1 to 4409\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hrs                      2917 non-null   float64\n",
      " 1   absences                 2860 non-null   float64\n",
      " 2   JobInvolvement           2888 non-null   float64\n",
      " 3   PerformanceRating        2834 non-null   float64\n",
      " 4   EnvironmentSatisfaction  2757 non-null   float64\n",
      " 5   JobSatisfaction          2914 non-null   float64\n",
      " 6   WorkLifeBalance          2891 non-null   float64\n",
      " 7   Age                      2892 non-null   float64\n",
      " 8   Attrition                3528 non-null   object \n",
      " 9   BusinessTravel           2929 non-null   object \n",
      " 10  Department               2861 non-null   object \n",
      " 11  DistanceFromHome         2975 non-null   float64\n",
      " 12  Education                2904 non-null   float64\n",
      " 13  EducationField           3528 non-null   object \n",
      " 14  EmployeeCount            2790 non-null   float64\n",
      " 15  EmployeeID               3528 non-null   int64  \n",
      " 16  Gender                   2816 non-null   object \n",
      " 17  JobLevel                 2926 non-null   float64\n",
      " 18  JobRole                  2896 non-null   object \n",
      " 19  MaritalStatus            2910 non-null   object \n",
      " 20  MonthlyIncome            2878 non-null   float64\n",
      " 21  NumCompaniesWorked       2757 non-null   float64\n",
      " 22  Over18                   2841 non-null   object \n",
      " 23  PercentSalaryHike        2916 non-null   float64\n",
      " 24  StandardHours            2797 non-null   float64\n",
      " 25  StockOptionLevel         2888 non-null   float64\n",
      " 26  TotalWorkingYears        2870 non-null   float64\n",
      " 27  TrainingTimesLastYear    2758 non-null   float64\n",
      " 28  YearsAtCompany           2940 non-null   float64\n",
      " 29  YearsSinceLastPromotion  2909 non-null   float64\n",
      " 30  YearsWithCurrManager     3528 non-null   int64  \n",
      "dtypes: float64(21), int64(2), object(8)\n",
      "memory usage: 882.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## 3.2. Missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist, we check the number the total number of missing values in the dataset in order to know if we have to clean the dataset or not.\n",
    "\n",
    "We use the train_set partition (and not X_train nor y_train) as it contains both the target variable and the rest of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hrs                        611\n",
      "absences                   668\n",
      "JobInvolvement             640\n",
      "PerformanceRating          694\n",
      "EnvironmentSatisfaction    771\n",
      "JobSatisfaction            614\n",
      "WorkLifeBalance            637\n",
      "Age                        636\n",
      "Attrition                    0\n",
      "BusinessTravel             599\n",
      "Department                 667\n",
      "DistanceFromHome           553\n",
      "Education                  624\n",
      "EducationField               0\n",
      "EmployeeCount              738\n",
      "EmployeeID                   0\n",
      "Gender                     712\n",
      "JobLevel                   602\n",
      "JobRole                    632\n",
      "MaritalStatus              618\n",
      "MonthlyIncome              650\n",
      "NumCompaniesWorked         771\n",
      "Over18                     687\n",
      "PercentSalaryHike          612\n",
      "StandardHours              731\n",
      "StockOptionLevel           640\n",
      "TotalWorkingYears          658\n",
      "TrainingTimesLastYear      770\n",
      "YearsAtCompany             588\n",
      "YearsSinceLastPromotion    619\n",
      "YearsWithCurrManager         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. Observations\n",
    "\n",
    "- Many variables have a significant number of missing values, ranging from 588 to over 771 missing values, which means that having 4410 rows in the dataset, we have a significant number of missing values ranging from 13.3% to 17.5% of the total number of rows in the dataset, which is a lot.\n",
    "  \n",
    "  These variables include: 'hrs', 'absences', 'JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance', 'Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'Education', 'EmployeeCount', 'Gender', 'JobLevel', 'JobRole', 'MaritalStatus', 'MonthlyIncome', 'NumCompaniesWorked', 'Over18', 'PercentSalaryHike', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', and 'YearsSinceLastPromotion'.\n",
    "\n",
    "- Some variables have no missing values, such as 'Attrition' (as it is the target variable), 'EducationField', and 'YearsWithCurrManager'.\n",
    "\n",
    "It's important to note that missing values can have an impact on the quality and reliability of the data and may require appropriate handling techniques, such as imputation or deletion, depending on the specific analysis or modeling objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting outliers in a dataset before training a model is crucial because outliers can significantly affect the performance and accuracy of the model. Outliers are data points that deviate significantly from the rest of the dataset and can cause the model to learn incorrect patterns and relationships. When outliers are present in the data, they can also increase the variance of the model, which can result in overfitting. Overfitting occurs when the model fits too closely to the training data, leading to poor generalization to new data. Therefore, it is important to detect and handle outliers properly to ensure the model's accuracy and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are getting information about the correlation of the variables between them. This information is valuable in order to make good decisions when deleting redundant attributes. Also note we are getting information about the correlation between each attribute and the solution variable. This allows us to know the most relevant attributes, making the best decisions when creating the different models.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Train-Test division \n",
    "\n",
    "Since we are working with a time dependent data, we need to avoid mixing it. Also, we are required to add the first 10 years of data to the train set and the last 2 years to the test set. This means we are assigning a 83.333333 percent of the data to train and a 16.66666666 to test.\n",
    "\n",
    "**Note**: This division was already done before the EDA. We overwrite it to start from a clean state.\n",
    "\n",
    "<mark>En esta práctica la evaluación será más sencilla que en la primera. Simplemente \n",
    "dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-\n",
    "parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train-Test RMSE and MAE function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Print model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions for all models \n",
    "\n",
    "For each possible method we have created two different models; One with predefined parameters and the second one with selected parameters. For each model we create a pipeline which includes the escaler ( except for trees and related ) and the model.  Note that we have selected RobustEscaler as our scaling method since we have found several outliers in the EDA. Secondly, we duplicate this two models per method and we add the selection of attributes. Note that the model with no selection of attributes and the one with selection of attributes have a double pipeline. Is a double pipeline since we use the output of the first pipeline ( best hiper-parameters ) directly into the second pipeline in order to avoid innecesary computing cost.\n",
    "\n",
    "We have decided to train all models in the most similar way possible in order for the results to be comparable. This way, all models with selected parameters use RandomSearch in order to avoid unnecessary computational cost while still producing good results. Secondly, we have decided to use TimeSeriesSplit, which is a useful method when working with time-related data. We also perform a cross-validation within the parameter search in order to avoid optimistic scoring for some parameters. For all models, we are using a 5-fold cross-validation. We also decided to use NMAE as our method for testing error since it provides an easy-to-understand score and reduces the weight of outliers (as observed during the EDA process).\n",
    "\n",
    "In addition note that in order to create the predefined models we are using gridsearch with just one option in the param-grid. This help us stay consistent in the way we create and compare models, since it provides a way of using cross-validation within the function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model construction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(1.3 puntos) Construcción de modelos: para esta práctica usaremos\n",
    "LogisticRegression como método base (sin ajustar hiper-parámetros) y Boosting\n",
    "como método avanzado (ajustando hiper-parámetros), a elegir. Es importante \n",
    "realizar los preprocesos que los datos necesiten, usando preferentemente \n",
    "pipelines. Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionale</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0.8 puntos) Usando algún método de selección de atributos de tipo filter\n",
    "(SelectKBest) de entre los disponibles en sklearn (f_classif, \n",
    "mutual_info_classif o chi2), comprobad si se pueden mejorar los resultados del \n",
    "apartado anterior y extraer conclusiones sobre qué atributos son más importantes, \n",
    "al menos de acuerdo a estos métodos</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logistic Regression\n",
    "Logistic regression with no hyperparameter tuning. It will be used as a baseline for the rest of the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Logistic Regression - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.1. Logistic Regression - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.2. Logistic Regression - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Boosting\n",
    "With <mark>(?)</mark> and without hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionales.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Boosting - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.1. Boosting - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.2. Boosting - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Boosting - Selected parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.1. Boosting - Selected parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.2. Boosting - Selected parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Código en un notebook. Es necesario que a lo largo de la práctica se vayan extrayendo \n",
    "conclusiones, y al final de la práctica, hay que hacer un resumen de todos los resultados \n",
    "obtenidos, usando tablas y/o gráficos.\n",
    "● El archivo conteniendo el mejor modelo obtenido (llamado «modelo_final.pkl»).</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Best Model Prediction - Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Selected Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Selected Model Prediction and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Selected Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this project, we have had the opportunity to gain a deeper understanding of the model selection process. We began with exploratory data analysis (EDA), which helped us to improve our understanding and management of the data. We found this to be an extremely useful tool throughout the entire project. We believe that this part of the project should be evaluated with greater emphasis, as it is the foundation upon which all of our decisions were based.\n",
    "\n",
    "Next, we created and trained all of our models, gaining experience in the use of pipelines and a deeper understanding of the importance of hyperparameters. Finally, we analyzed the different results provided by each model, gaining a better understanding of their respective advantages and disadvantages in terms of scoring and time.\n",
    "\n",
    "We believe that this project is an excellent complement to the main lessons, as it provides a deeper understanding of the subject matter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# X. Output the Jupyter Notebook as an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Export the notebook to HTML\n",
    "os.system(\"jupyter nbconvert --to html model.ipynb --output ../data/html/model.html\")\n",
    "print(\"Notebook exported to HTML\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_practica_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
