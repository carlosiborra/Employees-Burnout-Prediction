{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1. Aprendizaje Automático\n",
    "\n",
    "Authors: Carlos Iborra Llopis (100451170), Alejandra Galán Arrospide (100451273) <br>\n",
    "For additional notes and requirements: https://github.com/carlosiborra/Grupo02-Practica2-AprendizajeAutomatico\n",
    "\n",
    "❗If you are willing to run the code yourself, please clone the full GitHub repository, as it contains the necessary folder structures to export images and results❗"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Table of contents\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing necessary libraries \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from matplotlib.cbook import boxplot_stats as bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Cleaning ../data/img/ folder\n",
    "This way we avoid creating multiple images and sending the old ones to the trash.<br>\n",
    "Also using this to upload cleaner commits to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cleaning the ../data/img/ folder \"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"../data/img/*\")\n",
    "for f in files:\n",
    "    if os.path.isfile(f) and f.endswith(\".png\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Reading the datasets\n",
    "Reading the datasets from the bz2 files, group 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/attrition_available_2.pkl', 'rb') as archivo_pkl:\n",
    "    datos = pickle.load(archivo_pkl)\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to a csv file\n",
    "# datos.to_csv('../data/attrition_available_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0,15 puntos) Hacer un EDA muy simplificado: cuántas instancias / cuantos \n",
    "atributos y de qué tipo (numéricos, ordinales, categóricos); columnas constantes o \n",
    "innecesarias; que proporción de missing values por atributo; tipo de problema: \n",
    "(clasificación o regresión); ¿es desbalanceado?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts of Exploratory Data Analysis**\n",
    "\n",
    "- **2 types of Data Analysis**\n",
    "  - Confirmatory Data Analysis\n",
    "  - Exploratory Data Analysis\n",
    "- **4 Objectives of EDA**\n",
    "  - Discover Patterns\n",
    "  - Spot Anomalies\n",
    "  - Frame Hypothesis\n",
    "  - Check Assumptions\n",
    "- **2 methods for exploration**\n",
    "  - Univariate Analysis\n",
    "  - Bivariate Analysis\n",
    "- **Stuff done during EDA**\n",
    "  - Trends\n",
    "  - Distribution\n",
    "  - Mean\n",
    "  - Median\n",
    "  - Outlier\n",
    "  - Spread measurement (SD)\n",
    "  - Correlations\n",
    "  - Hypothesis testing\n",
    "  - Visual Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Dataset preparation\n",
    "\n",
    "To conduct exploratory data analysis (EDA) on our real data, we need to prepare the data first. Therefore, we have decided to separate the data into training and test sets at an early stage to avoid data leakage, which could result in an overly optimistic evaluation of the model, among other consequences. This separation will be done by dividing the data prematurely into training and test sets since potential data leakage can occur from the usage of the test partition, especially when including the result variable.\n",
    "\n",
    "It is important to note that this step is necessary because all the information obtained in this section will be used to make decisions such as dimensionality reduction. Furthermore, this approach makes the evaluation more realistic and rigorous since the test set is not used until the end of the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Simplemente dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-\n",
    "Aprendizaje Automático\n",
    "Práctica 2: Predicción de burnout\n",
    "parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filas = len(datos)\n",
    "\n",
    "\n",
    "print(\"La variable datos tiene\", num_filas - 1, \"atributos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atributos  = len(datos.keys())\n",
    "# -1 Since it includes the result column\n",
    "print(\"La variable datos tiene\", num_atributos -1, \"atributos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset and problem description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **hrs**: The number of hours worked by the employee (float64)\n",
    "- **absences**: The number of absences taken by the employee (float64)\n",
    "- **JobInvolvement**: The level of involvement the employee has in their job (float64)\n",
    "- **PerformanceRating**: The employee's performance rating (float64)\n",
    "- **EnvironmentSatisfaction**: The level of satisfaction the employee has with their work environment (float64)\n",
    "- **JobSatisfaction**: The level of satisfaction the employee has with their job (float64)\n",
    "- **WorkLifeBalance**: The balance between work and personal life for the employee (float64)\n",
    "- **Age**: The age of the employee (float64)\n",
    "- **Attrition**: Whether the employee has left the company or not (object)\n",
    "- **BusinessTravel**: The frequency of the employee's business travel (object)\n",
    "- **Department**: The department the employee works in (object)\n",
    "- **DistanceFromHome**: The distance from the employee's home to their workplace (float64)\n",
    "- **Education**: The highest level of education attained by the employee (int64)\n",
    "- **EducationField**: The field of study the employee specialized in (object)\n",
    "- **EmployeeCount**: The number of employees in the company (float64)\n",
    "- **EmployeeID**: A unique identifier for each employee (int64)\n",
    "- **Gender**: The gender of the employee (object)\n",
    "- **JobLevel**: The employee's job level in the company hierarchy (float64)\n",
    "- **JobRole**: The specific role the employee has in their department (object)\n",
    "- **MaritalStatus**: The employee's marital status (object)\n",
    "- **MonthlyIncome**: The employee's monthly income (float64)\n",
    "- **NumCompaniesWorked**: The number of companies the employee has worked for before joining the current company (float64)\n",
    "- **Over18**: Whether the employee is over 18 years old (presumably all employees are) (object)\n",
    "- **PercentSalaryHike**: The percentage of salary increase the employee received in their last salary hike (float64)\n",
    "- **StandardHours**: The standard number of working hours in the company (float64)\n",
    "- **StockOptionLevel**: The level of stock option the employee has (float64)\n",
    "- **TotalWorkingYears**: The total number of years the employee has worked (float64)\n",
    "- **Aprendizaje Automático**: Práctica 2: Predicción de burnout\n",
    "- **TrainingTimesLastYear**: The number of times the employee received training in the last year (float64)\n",
    "- **YearsAtCompany**: The number of years the employee has been with the company (float64)\n",
    "- **YearsSinceLastPromotion**: The number of years since the employee's last promotion (float64)\n",
    "- **YearsWithCurrManager**: The number of years the employee has been with their current manager (float64)\n",
    "\n",
    "**Note**: the values in between parenthesis correspond to the value type of each column in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist, we check the number the total number of missing values in the dataset in order to know if we have to clean the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting outliers in a dataset before training a model is crucial because outliers can significantly affect the performance and accuracy of the model. Outliers are data points that deviate significantly from the rest of the dataset and can cause the model to learn incorrect patterns and relationships. When outliers are present in the data, they can also increase the variance of the model, which can result in overfitting. Overfitting occurs when the model fits too closely to the training data, leading to poor generalization to new data. Therefore, it is important to detect and handle outliers properly to ensure the model's accuracy and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are getting information about the correlation of the variables between them. This information is valuable in order to make good decisions when deleting redundant attributes. Also note we are getting information about the correlation between each attribute and the solution variable. This allows us to know the most relevant attributes, making the best decisions when creating the different models.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Train-Test division \n",
    "\n",
    "Since we are working with a time dependent data, we need to avoid mixing it. Also, we are required to add the first 10 years of data to the train set and the last 2 years to the test set. This means we are assigning a 83.333333 percent of the data to train and a 16.66666666 to test.\n",
    "\n",
    "**Note**: This division was already done before the EDA. We overwrite it to start from a clean state.\n",
    "\n",
    "<mark>En esta práctica la evaluación será más sencilla que en la primera. Simplemente \n",
    "dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-\n",
    "parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train-Test RMSE and MAE function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Print model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions for all models \n",
    "\n",
    "For each possible method we have created two different models; One with predefined parameters and the second one with selected parameters. For each model we create a pipeline which includes the escaler ( except for trees and related ) and the model.  Note that we have selected RobustEscaler as our scaling method since we have found several outliers in the EDA. Secondly, we duplicate this two models per method and we add the selection of attributes. Note that the model with no selection of attributes and the one with selection of attributes have a double pipeline. Is a double pipeline since we use the output of the first pipeline ( best hiper-parameters ) directly into the second pipeline in order to avoid innecesary computing cost.\n",
    "\n",
    "We have decided to train all models in the most similar way possible in order for the results to be comparable. This way, all models with selected parameters use RandomSearch in order to avoid unnecessary computational cost while still producing good results. Secondly, we have decided to use TimeSeriesSplit, which is a useful method when working with time-related data. We also perform a cross-validation within the parameter search in order to avoid optimistic scoring for some parameters. For all models, we are using a 5-fold cross-validation. We also decided to use NMAE as our method for testing error since it provides an easy-to-understand score and reduces the weight of outliers (as observed during the EDA process).\n",
    "\n",
    "In addition note that in order to create the predefined models we are using gridsearch with just one option in the param-grid. This help us stay consistent in the way we create and compare models, since it provides a way of using cross-validation within the function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model construction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(1.3 puntos) Construcción de modelos: para esta práctica usaremos\n",
    "LogisticRegression como método base (sin ajustar hiper-parámetros) y Boosting\n",
    "como método avanzado (ajustando hiper-parámetros), a elegir. Es importante \n",
    "realizar los preprocesos que los datos necesiten, usando preferentemente \n",
    "pipelines. Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionale</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0.8 puntos) Usando algún método de selección de atributos de tipo filter\n",
    "(SelectKBest) de entre los disponibles en sklearn (f_classif, \n",
    "mutual_info_classif o chi2), comprobad si se pueden mejorar los resultados del \n",
    "apartado anterior y extraer conclusiones sobre qué atributos son más importantes, \n",
    "al menos de acuerdo a estos métodos</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logistic Regression\n",
    "Logistic regression with no hyperparameter tuning. It will be used as a baseline for the rest of the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Logistic Regression - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.1. Logistic Regression - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.2. Logistic Regression - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Boosting\n",
    "With <mark>(?)</mark> and without hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionales.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Boosting - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.1. Boosting - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.2. Boosting - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Boosting - Selected parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.1. Boosting - Selected parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.2. Boosting - Selected parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Código en un notebook. Es necesario que a lo largo de la práctica se vayan extrayendo \n",
    "conclusiones, y al final de la práctica, hay que hacer un resumen de todos los resultados \n",
    "obtenidos, usando tablas y/o gráficos.\n",
    "● El archivo conteniendo el mejor modelo obtenido (llamado «modelo_final.pkl»).</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Model evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Best Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Best Model Prediction - Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Selected Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Selected Model Prediction and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Selected Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this project, we have had the opportunity to gain a deeper understanding of the model selection process. We began with exploratory data analysis (EDA), which helped us to improve our understanding and management of the data. We found this to be an extremely useful tool throughout the entire project. We believe that this part of the project should be evaluated with greater emphasis, as it is the foundation upon which all of our decisions were based.\n",
    "\n",
    "Next, we created and trained all of our models, gaining experience in the use of pipelines and a deeper understanding of the importance of hyperparameters. Finally, we analyzed the different results provided by each model, gaining a better understanding of their respective advantages and disadvantages in terms of scoring and time.\n",
    "\n",
    "We believe that this project is an excellent complement to the main lessons, as it provides a deeper understanding of the subject matter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# X. Output the Jupyter Notebook as an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Export the notebook to HTML\n",
    "os.system(\"jupyter nbconvert --to html model.ipynb --output ../data/html/model.html\")\n",
    "print(\"Notebook exported to HTML\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_practica_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
