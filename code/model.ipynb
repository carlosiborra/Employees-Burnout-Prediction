{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1. Aprendizaje Automático\n",
    "\n",
    "Authors: Carlos Iborra Llopis (100451170), Alejandra Galán Arrospide (100451273) <br>\n",
    "For additional notes and requirements: https://github.com/carlosiborra/Grupo02-Practica2-AprendizajeAutomatico\n",
    "\n",
    "❗If you are willing to run the code yourself, please clone the full GitHub repository, as it contains the necessary folder structures to export images and results❗"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Table of contents\n",
    "\n",
    "- [Práctica 1. Aprendizaje Automático](#práctica-1-aprendizaje-automático)\n",
    "  - [0. Table of contents](#0-table-of-contents)\n",
    "  - [1. Requirements](#1-requirements)\n",
    "  - [2. Reading the datasets](#2-reading-the-datasets)\n",
    "  - [3. Exploratory Data Analysis](#3-EDA)\n",
    "  - [4. Train-Test division ](#4-Train-Test-division )\n",
    "  - [5. Basic Methods](#5-basic-methods)\n",
    "  - [6. Reducing Dimensionality](#6-Reducing-Dimensionality)\n",
    "  - [7. Advanced methods ](#7-Advanced-methods )\n",
    "  - [8. Best model](#8-Best-model)\n",
    "  - [9. Final Conclusions](#6-Final-Conclusions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importing necessary libraries \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from matplotlib.cbook import boxplot_stats as bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Cleaning ../data/img/ folder\n",
    "This way we avoid creating multiple images and sending the old ones to the trash.<br>\n",
    "Also using this to upload cleaner commits to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cleaning the ../data/img/ folder \"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"../data/img/*\")\n",
    "for f in files:\n",
    "    if os.path.isfile(f) and f.endswith(\".png\"):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Reading the datasets\n",
    "Reading the datasets from the bz2 files, group 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/attrition_available_2.pkl', 'rb') as archivo_pkl:\n",
    "    datos = pickle.load(archivo_pkl)\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the data to a csv file\n",
    "# datos.to_csv('../data/attrition_available_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0,15 puntos) Hacer un EDA muy simplificado: cuántas instancias / cuantos \n",
    "atributos y de qué tipo (numéricos, ordinales, categóricos); columnas constantes o \n",
    "innecesarias; que proporción de missing values por atributo; tipo de problema: \n",
    "(clasificación o regresión); ¿es desbalanceado?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts of Exploratory Data Analysis**\n",
    "\n",
    "- **2 types of Data Analysis**\n",
    "  - Confirmatory Data Analysis\n",
    "  - Exploratory Data Analysis\n",
    "- **4 Objectives of EDA**\n",
    "  - Discover Patterns\n",
    "  - Spot Anomalies\n",
    "  - Frame Hypothesis\n",
    "  - Check Assumptions\n",
    "- **2 methods for exploration**\n",
    "  - Univariate Analysis\n",
    "  - Bivariate Analysis\n",
    "- **Stuff done during EDA**\n",
    "  - Trends\n",
    "  - Distribution\n",
    "  - Mean\n",
    "  - Median\n",
    "  - Outlier\n",
    "  - Spread measurement (SD)\n",
    "  - Correlations\n",
    "  - Hypothesis testing\n",
    "  - Visual Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Dataset preparation\n",
    "\n",
    "To conduct exploratory data analysis (EDA) on our real data, we need to prepare the data first. Therefore, we have decided to separate the data into training and test sets at an early stage to avoid data leakage, which could result in an overly optimistic evaluation of the model, among other consequences. This separation will be done by dividing the data prematurely into training and test sets since potential data leakage can occur from the usage of the test partition, especially when including the result variable.\n",
    "\n",
    "It is important to note that this step is necessary because all the information obtained in this section will be used to make decisions such as dimensionality reduction. Furthermore, this approach makes the evaluation more realistic and rigorous since the test set is not used until the end of the process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Simplemente dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-\n",
    "Aprendizaje Automático\n",
    "Práctica 2: Predicción de burnout\n",
    "parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La variable datos tiene 4409 atributos.\n"
     ]
    }
   ],
   "source": [
    "num_filas = len(datos)\n",
    "\n",
    "\n",
    "print(\"La variable datos tiene\", num_filas - 1, \"atributos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La variable datos tiene 30 atributos.\n"
     ]
    }
   ],
   "source": [
    "num_atributos  = len(datos.keys())\n",
    "# -1 Since it includes the result column\n",
    "print(\"La variable datos tiene\", num_atributos -1, \"atributos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4410 entries, 1 to 4409\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   hrs                      3639 non-null   float64\n",
      " 1   absences                 3575 non-null   float64\n",
      " 2   JobInvolvement           3585 non-null   float64\n",
      " 3   PerformanceRating        3534 non-null   float64\n",
      " 4   EnvironmentSatisfaction  3428 non-null   float64\n",
      " 5   JobSatisfaction          3637 non-null   float64\n",
      " 6   WorkLifeBalance          3620 non-null   float64\n",
      " 7   Age                      3636 non-null   float64\n",
      " 8   Attrition                4410 non-null   object \n",
      " 9   BusinessTravel           3687 non-null   object \n",
      " 10  Department               3575 non-null   object \n",
      " 11  DistanceFromHome         3681 non-null   float64\n",
      " 12  Education                3628 non-null   float64\n",
      " 13  EducationField           4410 non-null   object \n",
      " 14  EmployeeCount            3484 non-null   float64\n",
      " 15  EmployeeID               4410 non-null   int64  \n",
      " 16  Gender                   3523 non-null   object \n",
      " 17  JobLevel                 3641 non-null   float64\n",
      " 18  JobRole                  3625 non-null   object \n",
      " 19  MaritalStatus            3639 non-null   object \n",
      " 20  MonthlyIncome            3579 non-null   float64\n",
      " 21  NumCompaniesWorked       3465 non-null   float64\n",
      " 22  Over18                   3551 non-null   object \n",
      " 23  PercentSalaryHike        3657 non-null   float64\n",
      " 24  StandardHours            3491 non-null   float64\n",
      " 25  StockOptionLevel         3609 non-null   float64\n",
      " 26  TotalWorkingYears        3596 non-null   float64\n",
      " 27  TrainingTimesLastYear    3476 non-null   float64\n",
      " 28  YearsAtCompany           3678 non-null   float64\n",
      " 29  YearsSinceLastPromotion  3628 non-null   float64\n",
      " 30  YearsWithCurrManager     4410 non-null   int64  \n",
      "dtypes: float64(21), int64(2), object(8)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "datos.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist, we check the number the total number of missing values in the dataset in order to know if we have to clean the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hrs                        771\n",
       "absences                   835\n",
       "JobInvolvement             825\n",
       "PerformanceRating          876\n",
       "EnvironmentSatisfaction    982\n",
       "JobSatisfaction            773\n",
       "WorkLifeBalance            790\n",
       "Age                        774\n",
       "Attrition                    0\n",
       "BusinessTravel             723\n",
       "Department                 835\n",
       "DistanceFromHome           729\n",
       "Education                  782\n",
       "EducationField               0\n",
       "EmployeeCount              926\n",
       "EmployeeID                   0\n",
       "Gender                     887\n",
       "JobLevel                   769\n",
       "JobRole                    785\n",
       "MaritalStatus              771\n",
       "MonthlyIncome              831\n",
       "NumCompaniesWorked         945\n",
       "Over18                     859\n",
       "PercentSalaryHike          753\n",
       "StandardHours              919\n",
       "StockOptionLevel           801\n",
       "TotalWorkingYears          814\n",
       "TrainingTimesLastYear      934\n",
       "YearsAtCompany             732\n",
       "YearsSinceLastPromotion    782\n",
       "YearsWithCurrManager         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Train-Test division \n",
    "\n",
    "Since we are working with a time dependent data, we need to avoid mixing it. Also, we are required to add the first 10 years of data to the train set and the last 2 years to the test set. This means we are assigning a 83.333333 percent of the data to train and a 16.66666666 to test.\n",
    "\n",
    "**Note**: This division was already done before the EDA. We overwrite it to start from a clean state.\n",
    "\n",
    "<mark>En esta práctica la evaluación será más sencilla que en la primera. Simplemente \n",
    "dividiremos los datos en un conjunto de train para entrenar y ajustar hiper-\n",
    "parámetros, y un conjunto de test en el que evaluaremos las distintas posibilidades \n",
    "que se probarán en la práctica. Hay que recordar que En problemas de clasificación \n",
    "desbalanceados hay que usar particiones estratificadas y métricas adecuadas \n",
    "(balanced_accuracy, f1, matriz de confusión). También es conveniente que los \n",
    "métodos de construcción de modelos traten el desbalanceo, usando por ejemplo \n",
    "el parámetro class_weight=”balanced”.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train-Test RMSE and MAE function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Print model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Validation splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions for all models \n",
    "\n",
    "For each possible method we have created two different models; One with predefined parameters and the second one with selected parameters. For each model we create a pipeline which includes the escaler ( except for trees and related ) and the model.  Note that we have selected RobustEscaler as our scaling method since we have found several outliers in the EDA. Secondly, we duplicate this two models per method and we add the selection of attributes. Note that the model with no selection of attributes and the one with selection of attributes have a double pipeline. Is a double pipeline since we use the output of the first pipeline ( best hiper-parameters ) directly into the second pipeline in order to avoid innecesary computing cost.\n",
    "\n",
    "We have decided to train all models in the most similar way possible in order for the results to be comparable. This way, all models with selected parameters use RandomSearch in order to avoid unnecessary computational cost while still producing good results. Secondly, we have decided to use TimeSeriesSplit, which is a useful method when working with time-related data. We also perform a cross-validation within the parameter search in order to avoid optimistic scoring for some parameters. For all models, we are using a 5-fold cross-validation. We also decided to use NMAE as our method for testing error since it provides an easy-to-understand score and reduces the weight of outliers (as observed during the EDA process).\n",
    "\n",
    "In addition note that in order to create the predefined models we are using gridsearch with just one option in the param-grid. This help us stay consistent in the way we create and compare models, since it provides a way of using cross-validation within the function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model construction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(1.3 puntos) Construcción de modelos: para esta práctica usaremos\n",
    "LogisticRegression como método base (sin ajustar hiper-parámetros) y Boosting\n",
    "como método avanzado (ajustando hiper-parámetros), a elegir. Es importante \n",
    "realizar los preprocesos que los datos necesiten, usando preferentemente \n",
    "pipelines. Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionale</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>(0.8 puntos) Usando algún método de selección de atributos de tipo filter\n",
    "(SelectKBest) de entre los disponibles en sklearn (f_classif, \n",
    "mutual_info_classif o chi2), comprobad si se pueden mejorar los resultados del \n",
    "apartado anterior y extraer conclusiones sobre qué atributos son más importantes, \n",
    "al menos de acuerdo a estos métodos</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Logistic Regression\n",
    "Logistic regression with no hyperparameter tuning. It will be used as a baseline for the rest of the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Logistic Regression - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.1. Logistic Regression - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.2. Logistic Regression - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Boosting\n",
    "With <mark>(?)</mark> and without hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Como método de boosting, se puede elegir uno de entre los métodos de \n",
    "boosting disponibles en scikit-learn. Si además se usa uno de entre las librerías \n",
    "externas xgboost, lightgbm o catboost, se pueden sacar +0.35 puntos adicionales.</mark>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Boosting - Predefined parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.1. Boosting - Predefined parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.2. Boosting - Predefined parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Boosting - Selected parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.1. Boosting - Selected parameters - No attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.2. Boosting - Selected parameters - Attribute selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# X. Output the Jupyter Notebook as an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook exported to HTML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/carlosiborra/anaconda3/envs/ml_practica_2/bin/jupyter-nbconvert\", line 6, in <module>\n",
      "    from nbconvert.nbconvertapp import main\n",
      "  File \"/home/carlosiborra/anaconda3/envs/ml_practica_2/lib/python3.10/site-packages/nbconvert/__init__.py\", line 4, in <module>\n",
      "    from .exporters import *\n",
      "  File \"/home/carlosiborra/anaconda3/envs/ml_practica_2/lib/python3.10/site-packages/nbconvert/exporters/__init__.py\", line 3, in <module>\n",
      "    from .html import HTMLExporter\n",
      "  File \"/home/carlosiborra/anaconda3/envs/ml_practica_2/lib/python3.10/site-packages/nbconvert/exporters/html.py\", line 11, in <module>\n",
      "    from jinja2 import contextfilter\n",
      "ImportError: cannot import name 'contextfilter' from 'jinja2' (/home/carlosiborra/anaconda3/envs/ml_practica_2/lib/python3.10/site-packages/jinja2/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Export the notebook to HTML\n",
    "os.system(\"jupyter nbconvert --to html model.ipynb --output ../data/html/model.html\")\n",
    "print(\"Notebook exported to HTML\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_practica_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
